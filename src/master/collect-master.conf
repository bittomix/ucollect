[main]
; How to connect to the DB
dbuser: updater
dbpasswd: 12345
db: ucollect
dbhost: localhost
; Port to listen on
port: 5678
port_compression: 5679
; The logging format. See http://docs.python.org/2/library/logging.html
log_format: %(name)s@%(module)s:%(lineno)s	%(asctime)s	%(levelname)s	%(message)s
; Severity of the logs. One of TRACE, DEBUG, INFO, WARN, ERROR, CRITICAL
log_severity: TRACE
; Where to log. - means stderr.
log_file: master.log
; Maximum size of log file (in bytes)
log_file_size: 134217728
; Maximum number of backup log files when rotated
log_file_count: 5
; The SSL certificate
cert = /etc/ucollect-dev.pem
key = /etc/ucollect-dev.pem
; Where the authenticator lives
authenticator_host: localhost
authenticator_port: 8888
fastpings:
	0000000500000842

; The plugins to load follow. Each name is the class to load and instantiate.

[statetrans_plugin.StatetransPlugin]
; threshold 85 = 0.85
treshold = 85
; learn in miliseconds
learn = 30000
; interval in seconds
interval = 10

; DB query 1
score = 90
time = 24:00
clients = 2

; DB query 2
score2 = 95
time2 = 01:00
count2 = 100



[spoof_plugin.SpoofPlugin]

[count_plugin.CountPlugin]
; The plugin that counts some stuff (packets of various properties, amount of data, ...)
interval: 60 ; How often to store a snapshot, seconds.
aggregate_delay: 5 ; How long to wait for answers after sending the query to store data into DB.

[buckets.main.BucketsPlugin]
; This one hashes packets to several buckets and checks the bucket sizes look statistically similar.
; If some does not, it is called an anomaly and the packet keys (the hashed properties) are guessed.
bucket_count: 13 ; Number of hash buckets to hash into
hash_count: 5 ; Number of different hash functions, used to guess the right keys
; Whitespace separated list of criteria to use. Each is fully-qualified class name.
criteria: buckets.criterion.AddressAndPort
	buckets.criterion.Port
	buckets.criterion.Address
history_size: 1 ; Number of history snapshots back kept in clients, for asking for keys
; Maximum number of keys kept per history snapshot and criterion on each client.
; Limited so the memory doesn't grow without bounds.
max_key_count: 1000
granularity: 5 ; Number of seconds in each timeslot
max_timeslots: 30 ; Number of time slots allocated in client (if there are more than this during one interval, the client data will be discarded). Recommended is at least twice as much + a little, in case the server would skip a generation.
interval: 60 ; Number of seconds between requesting a snapshot from clients.
gather_history_max: 4 ; Number of snapshots kept back on server, used for computing the anomalies.
aggregate_delay: 5 ; How long to wait for answers from clients between working on them.
anomaly_threshold: 1.8 ; How sensitive to be about anomalies. Lower the number, more the anomalies.
config_version: 1 ; If you change the config, change this as well

[sniff.main.SniffPlugin]
taskers = sniff.cert.Cert
	sniff.ping.Pinger
	sniff.nat.Nat
parallel_limit = 20
task_timeout = 1
interval = 1
# 15 seconds between starting tasks, so they don't flood all at once
start_interval = 15
ping_interval = 3600
ping_batchsize = 20
cert_interval = 3600
cert_batchsize = 20

[bandwidth_plugin.BandwidthPlugin]
interval: 900 ; How often to store a snapshot, seconds.
aggregate_delay: 5 ; How long to wait for answers from clients before working on them.

[flow_plugin.FlowPlugin]
[fwup_plugin.FWUpPlugin]

[refused_plugin.RefusedPlugin]
version = 1
finished_limit = 10
send_limit = 3
undecided_limit = 50
timeout = 30000
max_age = 120000

[fake_plugin.FakePlugin]
version = 1
max_age = 60000
max_size = 2048
max_attempts = 2
throttle_holdback = 120000
